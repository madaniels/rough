Explain ETL life-cycle (https://www.red-gate.com/simple-talk/sql/database-delivery/database-lifecycle-management-for-etl-systems/)

Extract-Transform-Load or ETL refers to the process of transferring data in bulk from one system to another. 
ETL systems extract and transform large volumes of source data into a new dataset, 
and then load that data into a new data store, 
for subsequent querying. 
This process happens because we ingest data from a variety of sources (some internal, 
some external to the organization), 
and we need to reformat or regularize the data fields before we can do further processing downstream. 
Source data might include:

Order placement data from an e-commerce web application
Data from a customer engagement system such as a CRM like Salesforce
Anonymized patient care data from hospitals across one country or region
Customer financial data relating to credit-worthiness
It may come as data files from other applications, 
from open data sources on the internet, 
from services on the intranet or Cloud, 
or from a type of data warehouse.

ETL sounds like three distinct phases but in fact the phases overlap and it’s not necessarily a serial process. 
The ‘users’ of ETL systems are always other software systems or people involved in data analysis, 
rather than interactive customers or end-users.

The ETL process often combines data from multiple different providers, 
and in many cases an ETL destination can also be a data source for another ETL process. 
For example, 
the SSAS cube may be a data source for an upstream OLTP database, 
providing aggregated and calculated values for key business metrics, 
as the raw data rows are archived.

ETL Workflows
Compared to transactional systems, 
ETL systems do not really have an ongoing ‘live’ state, 
but instead typically operate on a data load cycle that might be daily, 
weekly, 
or monthly, 
usually containing a ‘critical’ period where data must be loaded within a specific timeframe in order to meet internal or external deadlines. 
For instance, 
one ETL team may have to process data for an internal BI system, 
delivering once a week for finance meeting of the executive team, 
whilst another ETL may need to meet a monthly deadline imposed by a market regulator to report on the previous month’s transactions.

Plan around data volume
One of the difficulties of many ETL processes is the volume of data that must be handled. 
The ETL team need to be able to estimate of the volume of data that the system needs to load now, 
and will need to load in a year from now, 
in order to plan for both current performance and scalability into the future. 
When ETL processes are chained, 
or run in parallel, 
a single process can, 
by running for an unexpectedly long time, 
affect other processes so that the total time period escalates.

Prevent excessive blocking/deadlocking between tasks
A fundamental challenge for ETL systems is that that they will often be running various tasks, 
with varying frequencies and durations. 
Some of these tasks need to run concurrently in order to complete the required processing in the allotted time window. 
Extraction tasks can run as frequently every 5 minutes depending on the reporting requirements. 
Likewise, 
transform- and load-related tasks will each need to run on a set schedule. 
Often, 
different tasks will compete for data in the same table. 
For example, 
at same time as importing today’s data into relational tables in the target database, 
another process may be reading data out of these same tables for export to an SSAS cube. 
In minimizing blocking as far as possible, 
timing and scheduling of the various tasks is crucial.
